# 하트도그널 - 유기견 매칭 서비스
사이언스 : 김경현, 김태훈, 이지훈 | 엔지니어 : 손지수(팀장), 이상엽, 이서정 <br>
웹사이트 : https://heartdognal.ml/

## 1. 프로젝트 개요

### 1-1 주제 선정 배경 및 목적
국민 4분의 1정도가 반려동물을 키우는 지금. 그만큼 버려지는 동물들도 많습니다. 하루에만 평균 370마리가 버려지고 그의 절반 가량이 죽음을 맞이한다고 합니다.
현재의 입양률은 30%대로 저조하고 이마저도 파양을 당하는 경우가 부지기수입니다. 저희는 입양희망자들이 나와 잘 맞는 아이는 어떤 특성을 가졌는지 파악하고 정보를 볼 수 있는 서비스가 있으면 좋겠다 생각했습니다. 

현재 운영되고 있는 유기견 서비스들은 정보 제공에 목적을 두고 있습니다. 국내에 몇 없는 매칭 서비스들은 품종을 추천하는 정도였습니다.
하지만 유기견의 80퍼센트 이상은 믹스견입니다. 저희는 품종견 뿐만 아니라 믹스견의 개별 특징을 파악해 입영희망자와 매칭 시키는 서비스를 만드는 것에 목적을 두고 있습니다.

### 1-2 프로젝트 수행 도구
협업툴 : trello / slack / docker / git <br>
기술스택 : hadoop / spark / oracle / airflow / django / kafka

### 1-3 워크플로우
![image](https://user-images.githubusercontent.com/72624263/203239444-7004e701-d069-454d-922c-0e24232ccb22.png)

### 1-4 서비스 구성도
![image](https://user-images.githubusercontent.com/72624263/203254279-6382d614-4a70-47fc-a35c-9d936f64fed3.png)


## 2. 데이터 파이프라인
### 2-1 수집 데이터
농림축산식품부 - 유기동물 API / 시도·시군구·보호소 API / 품종 API<br>
네이버 견종정보 지식백과 크롤링

### 2-2 데이터 웨어하우스
![image](https://user-images.githubusercontent.com/72624263/203210993-06d79d64-5581-4ecd-b568-61884945099c.png)
데이터 웨어하우스는 전사적인 측면에서 데이터를 수집하는 공간이기 때문에 데이터마트와 운영DB에 필요한 데이터들을 고려해 ERD를 구성했습니다

### 2-3 데이터 마트
![image](https://user-images.githubusercontent.com/72624263/203211130-ff89d313-40d0-4cb9-8ac3-4a0eec1be519.png)
데이터마트는 모델링에 필요한 유기견 데이터와 프로토타입 모델에서 필요한 지식백과 테이블을 transform하여 구성했습니다

### 2-4 운영 디비
![image](https://user-images.githubusercontent.com/72624263/203211324-3b9232a9-42c6-402b-ba18-45f47be07303.png)
운영디비는 유기견 테이블엔 군집화 라벨이 추가되고 그외엔 서비스에 필요한 시도 정보, 사용자 정보 그리고 사용자 활동 정보 테이블 등으로 구성되어 있습니다.

### 2-5 ETL
- 지식백과 ETL : 지식백과 크롤링 후 가공
- 보호소 ETL : 보호소 API와 유기견 API의 관계를 활용하여 제공해주지 않는 정보를 찾아내 가공. 각 데이터의 전화번호로 조인을 해서 참조하도록 ID 부여
- 유기견 ETL : 유기견 API에서 추출 후 가공

### 2-6 데이터 전처리
- 색상 전처리 : 색상 컬럼을 불러와서 단어들을 구분하고 중복횟수가 10 이상인 단어들만 파악하여 색상 사전을 만듭니다. 색상 정보는 크게 흰, 갈, 검, 금, 회색으로 나눌 수 있습니다. 이렇게 색상 사전에 따라 단어가 분류되면 해당 색을 포함한 색상컬럼을 새로 만듭니다.
- 특징 전처리 : 수집한 지식백과 데이터와 유기견 개별 특징의 단어들을 직접 비교해가며 특징 사전을 만들었습니다. 여기서 문제는 유기견 특징의 단어들이 사전에 있는 단어와 조금이라도 다르면 점수로 계산되지 않는다는 것에 있었습니다. 그래서 한국어 자연어 처리 모듈인 KoNLPy를 활용해 정형화를 하였습니다. 이 전처리로 평균 실루엣 점수 0.1을 증가시킬 수 있었습니다.
- 모델링 전처리 : 비정형데이터인 강아지 특징 컬럼에서 친화성 정도와 건강상태를 수치화 하기 위해 특징 단어사전에서 친화성, 건강 관련 단어가 얼마나 매치되는지 개수를 세고, 매우 같은 강조 부사를 통해 가중치를 부여하여 친화성지수와 건강지수를 계산하였습니다.
- 이상치 전처리 : 이상치는 일반적으로 모델링 성능을 떨어뜨리기 때문에 체중과 나이 컬럼에서 이상치를 제거했습니다.

### 2-7 배치
- Oracle scheduler : FINISHED_DOG(종료 유기견)과 비교하여 ROADDOG_INFO(모든 유기견)의 상태를 보호중에서 종료(반환/자연사/입양)로 업데이트 / 입양가능일로 부터 15일이 지난 경우 종료 처리
- Airflow : 데이터 파이프라인 자동화


## 3. 데이터 모델링
### 3-1 데이터 피처
유기견 입양에 대한 인식조사 및 동물보호 국민의식조사를 보면 유기견 입양에 있어서 건강상태, 친화성 등 나이와 성격이 중요함을 알 수 있었고 유기견 입양확률예측모형 논문에서 성별 및 중성화여부 또한 유기견 입양에 있어서 중요함을 알 수 있었습니다.

앞서 본 통계자료 및 논문을 바탕으로 유기견 입양에 있어 중요한 피쳐 7가지를 선별하였습니다.

이 중 수치형 변수인 체중, 나이, 친화성, 건강상태는 군집화, 콘텐츠기반 필터링 추천시스템에 사용하였으며 명목형 변수인 품종, 성별, 중성화여부는 웹페이지에 정보가 제공될 수 있도록 활용하였습니다.

### 3-2 클러스터링
군집화는 (입양희망자가 선호하는 반려견)에 대한 설문조사를 바탕으로 해당하는 군집의 유기견을 추천해줄 수 있도록 모델링하였습니다.

서비스 플로우에서 입양희망자는 로그인 후 설문조사를 통해 입양을 희망하는 강아지의 크기, 나이, 친화성정도, 건강상태 등을 설문조사하게됩니다. 설문조사 데이터를 바탕으로 해당 입양희망자에게 가장 잘 맞는 그룹의 유기견들이 선택되고 추천 페이지에는 선택된 유기견들이 보이게 됩니다.

- K-Means : 군집의 개수가 11개에서 30개인 모델의 기준으로 kmeans는 평균 0.5이상의 비교적 높은 실루엣 점수를 보였으며 군집의 개수가 25개 이상일 시 (군집의 데이터 및 실루엣 계수 분포) 역시 비교적 고른 결과를 보였습니다.
- 가우시안 혼합모델 : 평균 실루엣 점수가 0.25에서 0.44정도로 kmeans에 비해 많이 낮았으며 특정 군집에 데이터가 편중되거나 실루엣 점수 분포가 고르지 못한 결과를 보였습니다.
- DBSCAN : 주요 하이퍼파라미터인 입실론과 min_samples에 따른 평균 실루엣 점수는 대부분 0.3 미만으로 k-means와 가우시안 혼합모델에 비해 많이 낮은 결과를 보였습니다.

=> 최종 모델링 알고리즘 : K-Means

### 3-3 하이퍼파라미터 튜닝
K-Means의 가장 중요한 하이퍼파라미터인 클러스터링 갯수를 정하는 것이 모델링의 핵심적인 부분이였고 모델링 성능 뿐만 아니라 서비스 차원의 문제도 고려해야 했습니다. 
- 추천 강아지의 다양성 정도 : 추천 받는 강아지의 다양성을 높이려면 군집의 개수를 낮춰야되고, 설문조사에 정말 잘 맞는 강아지를 추천하려면 군집의 개수를 올려야되기 때문에 적절한 군집의 개수를 정하는 것이 중요하다고 볼 수 있겠습니다.
- 입양 가능한 일일 평균 유기동물 수 : 입양이 가능한 강아지는 하루 주기로 업데이트 되며 일일 평균 2000마리에서 2500마리인 것으로 확인되었습니다. 저희는 추천서비스의 품질을 위해서 각 군집 1개당 평균 강아지 수 100마리 이상의 강아지가 필요하다고 판단하였으며 그에 따른 적절한 군집의 수는 20개에서 25개 이하로 판단하였습니다.
- 엘보우 메소드와 실루엣 분석 : 엘보우 메소드는 군집의 개수가 17개 이상일 시 군집 내 변동성이 완만한 곡선 형태로 변화하며 실루엣 점수는 군집의 개수가 25개 이상일 시 0.5를 상회 하고 군집별 데이터 및 실루엣 계수 분포가 좋은 결과를 보였습니다.

=> 최종 클러스터링 개수 : 25개

### 3-4 데이터 스케일링
![image](https://user-images.githubusercontent.com/72624263/203216112-b6b5b11e-8fbc-440e-85a4-825f9a726ba6.png)
<br>전체적으로 스탠다드 스케일이 다른 두 스케일러에 비해 높은 성능을 보였습니다. <br>
![image](https://user-images.githubusercontent.com/72624263/203216220-a317415e-9b3b-45ee-8e33-952530f6c7c1.png)
<br>따라서 최종 군집화 모델에선 알고리즘은 K-Means, 데이터 스케일러는 standard, 하이퍼파라미터는 보시는 표와 같이 사용하여 유기견 데이터 약 2만건에 대한 학습을 진행하였습니다.

### 3-5 콘텐츠 기반 필터링 추천 시스템
저희는 쿠팡의 추천시스템에 착안하여 비슷한 형태의 추천시스템이 구현되도록 기획하였습니다. 설문조사를 바탕으로 추천된 군집의 유기견 중 한마리를 클릭하게 되면 코사인 유사도를 이용하여 만들어진 유기견 테이블에서 선택한 유기견과 유사한 강아지 5마리가 보여집니다. <br>

군집화에 의한 추천과 콘텐츠 기반 필터링 추천의 차이점은 군집화는 특정 군집 안에서 추천이 이루어지지만 콘텐츠 필터링 추천시스템은 군집과 상관 없이 선택된 강아지와 유사도가 높은순으로 추천이 이루어지게 됩니다.


## 4. 마무리
### 4-1 서비스 기대효과
입양희망자가 원하는 유기견을 분석해 정보를 제공하고 잘 맞는 유기견을 매칭함으로써 유기견 파양률 줄일 수 있습니다. 또한 서비스를 이용하는 사람이 많아지면 입양 문화 활성화 효과를 기대할 수 있습니다. 

### 4-2 향후 계획
- 현재 저희의 데이터파이프라인은 결합도가 높습니다. 시스템 아키텍처의 결합도를 낮추기 위해 rest_api 서버와 Kafka를 이용해 데이터 파이프라인이 특정 서비스와 직접적으로 연결 되지 않도록 만들어 줄 예정입니다. 
- 현재는 카프카가 웹 서비스와 연동되어 메시징 큐로서 로그를 수집하고 있습니다. 추후에는 수집된 로그로 테이블을 생성하여 분석 모델에 쓰일 수 있도록 데이터 파이프라인을 구축하겠습니다.
- 추천의 다양성과 추천의 질을 높이기 위해 유기강아지 데이터 셋의 털 색상과 털 상태의 피처를 추가해 모델링하여 기존의 추천서비스와 비교하여 적용해볼 예정입니다.
- 저희 서비스의 추천시스템 모델은 향후 일정 수 이상의 사용자 데이터 확보 시 콘텐츠 기반 추천시스템에서 콘텐츠 기반과 아이템 기반 협업 필터링이 결합된 하이브리드 추천시스템이 적용될 수 있도록 시스템을 구성할 계획입니다. 다수 사용자의 관심지수 데이터를 이용하여 사용자의 입양 선호도를 고려하고 더 폭 넓은 추천이 가능하게 될 것으로 예상됩니다.

### 4-3 개발후기 및 느낌점
김경현 : 많이 부족했지만 모르는 걸 물어봐도 하나하나 친절하게 답해주신 팀원분들과 강사님 두 분 덕에 그나마 따라갈 수 있었던 것 같습니다. 그동안 정말 고생하셨습니다.<br>
김태훈 : 이번 프로젝트를 하면서 전공과목(사이언스) 뿐만 아니라 프론트앤드 백앤드 까지 같이 배우는 시간이라 많은 것을 배울 수 있어서 유익했다. 모르는 부분이 많이 있어서 앞으로 좀 더 공부해야된다고 생각한다.<br>
손지수 : 이 과정에서 처음으로 프로젝트 팀장을 맡았는데 열심히 해준 팀원들에게 수고했다고 말씀 드리고 싶습니다. 여러 시행착오 끝에 서비스가 완성됐는데 힘들지만 성장할 수 있는 기회가 있어서 좋았습니다.<br>
이상엽 : 프로젝트를 하는 기간이 저는 많은 걸 배울수있는 시간이였다고 생각합니다 여전히 부족하지만 같이 프로젝트하고 얘기를 나눴던 사람들과 같이 일하기위해 더 열심히 공부해보겠습니다 진짜 재밌고 즐거웠습니다 감사합니다.<br>
이서정 : 이번 프로젝트를 통해 비정형 데이터를 다루면서 데이터 전처리, 구조화에 대해 고민할 수 있었고, 많은 문제를 해결하면서 한층 성장할 수 있는 시간이었습니다. 팀원들과 의견 대립과 수렴을 겪으면서 소통의 방법을 더 익힌 것 같고 프로젝트가 발전하는 모습을 볼 수 있어 재밌었습니다.<br>
이지훈 : 프로젝트를 시작하면서 데이터 사이언스도 어려운데 난생 처음 해보는 웹개발 까지 7주 안에 할 수 있을까 걱정이 컸습니다. 하지만 강사님들과 팀원들을 믿고 끝까지 포기하지말고 해보자 했는데 결국 프로젝트 마지막까지 올 수 있어서 기쁩니다.
